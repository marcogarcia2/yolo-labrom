import numpy as np
import cv2
import os
import random
import matplotlib.pyplot as plt
from pathlib import Path
import json
from tqdm import tqdm
import natsort
from ultralytics import YOLO
    

def generate_yolo_annotation(masks_paths: list, labels_path: str, class_id: int=0, epsilon: float=3.0):
    
    """
    Given a list of masks paths, generate YOLO annotations for segmentation and saves them in an output folder.
    """

    for mask_path in masks_paths:
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

        if mask is None:
            print(f"Error loading {mask_path}.")
            continue
        
        # Detect mask contours 
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if not contours:
            print(f"No contour found for mask {mask_path}.")
            continue

        for contour in contours:

            # Simplifies the contour using Ramer-Douglas-Peucker
            simplified_contour = cv2.approxPolyDP(contour, epsilon, True)

            # Normalizing the points to YOLO format
            h, w = mask.shape
            points = simplified_contour.reshape(-1, 2)  # Converter para lista de pontos (x, y)
        
            # Normalizing the coordinates [0,1]
            points_normalized = [(x / w, y / h) for x, y in points]
            
            # Generating the annotation itself
            annotation = f"{class_id} " + " ".join([f"{x:.6f} {y:.6f}" for x, y in points_normalized]) + "\n"

            # Saving it to the corresponding file
            label_name = os.path.splitext(os.path.basename(mask_path))[0].replace('_mask', '') + ".txt"
            with open(os.path.join(labels_path, label_name), "a") as f:
                f.write(annotation)


def polygon_visual_inspection(images_folder: str, labels_folder: str, n: int=5):
    
    """
    Visualizes the polygons generated by the labels N times. 
    """
    
    red = (0,0,255)

    images_list = [os.path.join(images_folder, f) for f in os.listdir(images_folder) if f.endswith((".jpg"))]
    selected_images = random.sample(images_list, n)

    for image_path in selected_images:
        
        label_path = image_path.replace(".jpg", ".txt").replace(images_folder, labels_folder)

        img = cv2.imread(image_path)

        if img is None:
            print(f"Error loading {image_path}.")
            continue

        labels = []
        with open (label_path, 'r') as label_file:
            lines = label_file.readlines()
            for line in lines:
                labels.append(line.strip())

        for label in labels:
            
            # Converting YOLO coordinates do pixels
            h, w, _ = img.shape
            
            values = label.split()
            values = list(map(float, values))

            points = np.array(values[1:])
            points = points.reshape(-1, 2)  # Transforming data to a list of points (x,y)
            points[:, 0] *= w  # Converting to real dimension
            points[:, 1] *= h  # Converting to real dimension

            thickness = int(0.005 * max(h,w))

            # Drawing the polygon
            points = points.astype(np.int32)
            cv2.polylines(img, [points], isClosed=True, color=red, thickness=thickness)
            for point in points:
                cv2.circle(img, point, thickness*2, red, -1)

        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        # Exibir a imagem com o polígono
        plt.imshow(img_rgb)
        plt.axis(False)
        plt.show()


def concat_videos(path1: str, path2: str):
    
    """
    Load and concatenate two videos into a single one, side-by-side.
    """

    # Private helper function that concatenates each frame individually
    def concat_frame(frame1, width1:int, frame2, width2:int, width:int, height:int, bar:int):
        
        # Creating a black (empty) frame
        frame = np.zeros((height, width, 3), dtype=np.uint8)

        # Coloring the namebar
        gray = (50,50,50)
        frame[0:bar, 0:width] = gray

        # Writing frame 1 at the left and frame 2 at the right
        frame[bar:bar+height1, 0:width1] = frame1
        frame[bar:bar+height2, width1:width1+width2] = frame2
        
        # Text parameters
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 2
        color = (255,255,255)
        thickness = 2

        name1 = "yolo11n"
        name2 = "yolo11s"

        cv2.putText(frame, name1, (0, bar//2), font, font_scale, color, thickness)
        cv2.putText(frame, name2, (width1, bar//2), font, font_scale, color, thickness)

        return frame


    cap1 = cv2.VideoCapture(path1)
    if not cap1.isOpened():
        print(f"Error opening video file: {path1}")
        return

    cap2 = cv2.VideoCapture(path2)
    if not cap2.isOpened():
        print(f"Error opening video file: {path2}")
        return

    
    # Getting info about video1
    fps1 = cap1.get(cv2.CAP_PROP_FPS)
    width1 = int(cap1.get(cv2.CAP_PROP_FRAME_WIDTH))
    height1 = int(cap1.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames1 = int(cap1.get(cv2.CAP_PROP_FRAME_COUNT)) # Opcional, para mostrar progresso
    print(f"Video 1: {width1}x{height1} @ {fps1:.2f} FPS, Total frames: {total_frames1}")
    
    # Getting info about video2
    fps2 = cap2.get(cv2.CAP_PROP_FPS)
    width2 = int(cap2.get(cv2.CAP_PROP_FRAME_WIDTH))
    height2 = int(cap2.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames2 = int(cap2.get(cv2.CAP_PROP_FRAME_COUNT)) # Opcional, para mostrar progresso
    print(f"Video 2: {width2}x{height2} @ {fps2:.2f} FPS, Total frames: {total_frames2}")

    # Security checking
    if fps1 != fps2: 
        print(f"Error: both videos must have the same FPS ({fps1} and {fps2}).")
        exit()

    if total_frames1 != total_frames2:
        print(f"Error: both videos must have the same amount of frames ({total_frames1} and {total_frames2}).")
        exit()

    # Creating directory or ignoring if it already exists
    try:
        os.makedirs("out", exist_ok=True)
    except OSError as e:
        print(f"Error creating directory 'out': {e}")

    # Creating the file name
    basename = os.path.basename(path1)
    out_path = "out/merged_" + basename

    # Removing previous video
    if os.path.exists(out_path):
        try:
            os.remove(out_path)
        except OSError as e:
            print(f"Error removing previous video '{out_path}': {e}")

    # Output video parameters
    bar = 100
    width = width1 + width2
    height = max(height1, height2) + bar
    fps = fps1
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(out_path, fourcc, fps, (width, height))

    # While loop that join and save frames 
    while cap1.isOpened() and cap2.isOpened():
        
        ret1, frame1 = cap1.read()        
        ret2, frame2 = cap2.read()        

        if not ret1 or not ret2:
            print("Successfully concatenated both videos.")
            break

        joined_frame = concat_frame(frame1, width1, frame2, width2, width, height, bar)
        if out is not None:
            out.write(joined_frame)

    cap1.release()
    cap2.release()
    if out is not None:
        out.release()

    print(out_path)
    return out_path




def create_video_from_predictions(model_path: str, frames_dir: str, output_path: str, fps: float, conf: float):
    """
    Run YOLO predictions in a sequence of frames and creates a video from its results.
    """

    # Loads the chosen YOLO model
    print(f"Loading model: {model_path}...")
    try:
        model = YOLO(model_path)
    except Exception as e:
        print(f"Error loading YOLO model: {e}")
        return

    # Find and sorts the frames
    print(f"Finding and sorting frames in: {frames_dir}...")
    try:
        frame_files = natsort.natsorted([p for p in frames_dir.glob("*.png")], key=lambda x: x.name)
        if not frame_files:
            print("Error: No .png file found in the specified directory.")
            return
    except FileNotFoundError:
        print(f"Error: Input directory not found: {frames_dir}")
        return

    print(f"{len(frame_files)} frames found.")

    # The first frame's dimensions will be the video dimensions 
    first_frame = cv2.imread(str(frame_files[0]))
    height, width, _ = first_frame.shape

    # Starts VideoWriter and sets .mp4 codec
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    video_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    if not video_writer.isOpened():
        print(f"Error: Couldn't write video file: {output_path}")
        return

    
    # Main loop: processing each frame
    for frame_path in tqdm(frame_files, desc="Processing Frames"):
        
        # Predicts current frame
        results = model.predict(source=str(frame_path), conf=conf, verbose=False)
        
        # Pega o frame resultante com as anotações desenhadas
        # results[0].plot() returns the image as an NumPy array in BGR color format (good for cv2)
        annotated_frame = results[0].plot()
        
        # Appends the frame to the video
        video_writer.write(annotated_frame)

    # Frees the video writer object and saves the file
    video_writer.release()
    print("\nProcess concluded.")
    print(f"Video saved in: {output_path}\n")

